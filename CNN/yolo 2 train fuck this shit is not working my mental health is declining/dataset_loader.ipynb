{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = r'C:\\Users\\Cordu\\Desktop\\Projects\\Artificial-Intelligence\\CNN\\Untitled Folder\\VOCdevkit\\VOC2007\\JPEGImages'\n",
    "IMG_TRAIN_TXT_PATH = r'C:\\Users\\Cordu\\Desktop\\Projects\\Artificial-Intelligence\\CNN\\Untitled Folder\\VOCdevkit\\VOC2007\\ImageSets\\Main\\train.txt'\n",
    "ANNOT_TRAIN_PATH = r'C:\\Users\\Cordu\\Desktop\\Projects\\Artificial-Intelligence\\CNN\\Untitled Folder\\VOCdevkit\\VOC2007\\Annotations'\n",
    "\n",
    "#TEST_PATH = 'C:\\Users\\Cordu\\Desktop\\Projects\\Artificial-Intelligence\\CNN\\Untitled Folder\\VOCdevkit\\VOC2007\\ImageSets\\Segmentation\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "'aeroplane',\n",
    "'bicycle',\n",
    "'bird',\n",
    "'boat',\n",
    "'bottle',\n",
    "'bus',\n",
    "'car',\n",
    "'cat',\n",
    "'chair',\n",
    "'cow',\n",
    "'diningtable',\n",
    "'dog',\n",
    "'horse',\n",
    "'motorbike',\n",
    "'person',\n",
    "'pottedplant',\n",
    "'sheep',\n",
    "'sofa',\n",
    "'train',\n",
    "'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, img_path, file_path, annot_path, img_size=416):\n",
    "        with open(file_path, 'r') as file:\n",
    "            self.img_files = [os.path.join(img_path, line.replace('\\n', '')) for line in file.readlines()]\n",
    "        \n",
    "        self.img_shape = (img_size, img_size)\n",
    "        self.max_objects = 75\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Get the image\n",
    "        img = cv2.imread(self.img_files[index] + '.jpg')\n",
    "        h, w, _ = img.shape\n",
    "        \n",
    "        # Dimension difference between height and width\n",
    "        dim_diff = np.abs(h - w)\n",
    "        \n",
    "        # Upper (left) and lower (right) padding\n",
    "        # [1, 2] padding means for [2, 2] ones matrix\n",
    "        # [0, 0, 0, 0, 0]\n",
    "        # [0, 1, 1, 0, 0]\n",
    "        # [0, 1, 1, 0, 0]\n",
    "        # [0, 0, 0, 0, 0]\n",
    "        # [0, 0, 0, 0, 0]\n",
    "        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
    "        \n",
    "        # Determine padding\n",
    "        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n",
    "        # Add padding\n",
    "        pad_img = np.pad(img, pad, 'constant', constant_values=128)\n",
    "        padded_h, padded_w, _ = pad_img.shape\n",
    "        \n",
    "        # Resize the image to the Darknet input dimension\n",
    "        pad_img = cv2.resize(pad_img, self.img_shape)\n",
    "        # Channels-first\n",
    "        input_img = pad_img.reshape((3, 416, 416))\n",
    "        # As pytorch tensor\n",
    "        input_img = torch.from_numpy(input_img).float().div(255.0)\n",
    "        \n",
    "        # -------------------\n",
    "        # -Label calculation-\n",
    "        # -------------------\n",
    "        \n",
    "        img_name = self.img_files[index].split('\\\\')[len(self.img_files[index].split('\\\\'))-1]\n",
    "        annot_file = os.path.join(ANNOT_TRAIN_PATH, img_name + '.xml')\n",
    "        \n",
    "        tree = ET.parse(annot_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        \n",
    "        filled_labels = np.zeros((self.max_objects, 5))\n",
    "        \n",
    "        for i, child in enumerate(root.iter('object')):\n",
    "            object_name = child.find('name').text\n",
    "            x_min = int(child.find('bndbox').find('xmin').text)\n",
    "            y_min = int(child.find('bndbox').find('ymin').text)\n",
    "            \n",
    "            x_max = int(child.find('bndbox').find('xmax').text)\n",
    "            y_max = int(child.find('bndbox').find('ymax').text)\n",
    "            \n",
    "            # Ajust for the added padding\n",
    "            x_min += pad[1][0]\n",
    "            y_min += pad[0][0]\n",
    "            x_max += pad[1][0]\n",
    "            y_max += pad[0][0]\n",
    "            \n",
    "            x = (x_min + x_max)/2. * 1./padded_w\n",
    "            y = (y_min + y_max)/2. * 1./padded_h\n",
    "            w = (x_max - x_min) * 1./padded_w\n",
    "            h = (y_max - y_min) * 1./padded_h\n",
    "            \n",
    "            filled_labels[i] = np.array([classes.index(object_name), x, y, w, h])\n",
    "        \n",
    "        filled_labels = torch.from_numpy(filled_labels)\n",
    "        \n",
    "        return input_img, filled_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded files is 16551\n"
     ]
    }
   ],
   "source": [
    "#dataset = VOCDataset(IMG_PATH, IMG_TRAIN_TXT_PATH, ANNOT_TRAIN_PATH)\n",
    "#print(\"Number of loaded files is {}\".format(len(dataset.img_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 416])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "#inp, target = next(iter(dataset))\n",
    "#print(inp.shape)\n",
    "#print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
