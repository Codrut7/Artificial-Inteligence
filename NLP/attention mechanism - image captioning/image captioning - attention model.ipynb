{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size, dropout=0.10, trainCNN = False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # Define the embedding size used in the output\n",
    "        self.output_size = output_size\n",
    "        # Resnet model\n",
    "        # Remove the fc layer and the avg pool layer\n",
    "        self.cnn = nn.Sequential(*list(models.resnet18(pretrained=True).children())[:-2])\n",
    "        # Define the output linear transformation, relu, dropout CNN output -> out_size\n",
    "        self.out = nn.Linear(output_size , output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Replace the last linear transf of the CNN with the custom linear transf\n",
    "        # Lock CNN training parameters if trainCNN = False\n",
    "        self.lock() if not trainCNN else None\n",
    "        \n",
    "    def lock(self):\n",
    "        for parameter in self.cnn.parameters():\n",
    "            parameter.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_images):\n",
    "        \n",
    "        out = self.cnn(input_images)\n",
    "        # Transform the CNN output to [b, H, W, C] \n",
    "        out = out.permute(0, 3, 2, 1)\n",
    "        out = out.reshape(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "        \n",
    "        out = self.dropout(self.relu(self.out(out)))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttentionDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, dropout=0.10):\n",
    "        super(BahdanauAttentionDecoder, self).__init__()\n",
    "        \n",
    "        # Define the dimension size for the vocab, hidden and embedding\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the linear tranf for each score\n",
    "        self.Qattn = nn.Linear(self.embed_size, self.hidden_size, bias=False)\n",
    "        self.Kattn = nn.Linear(self.embed_size, self.hidden_size, bias=False)\n",
    "        self.Vattn = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        \n",
    "        self.out1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        # dropout, batchnorm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features=hidden_size)\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        # Sequential lstm\n",
    "        self.hidden_concat = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.input_lin = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # Out linear\n",
    "        self.out2 = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "         \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, inpt_sequence, image_features, hidden):\n",
    "    \n",
    "        # Score shape = [B, sequence_length, 1]\n",
    "        # Bahdau Fattn = Wv * tanh[ image_features(encoder_outputs kinda) * Wk + hidden * Wq ]\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        score = self.Vattn(torch.tanh(self.Kattn(image_features) + self.Qattn(hidden)))\n",
    "        \n",
    "        # Softmax this beautiful score\n",
    "        attn = nn.functional.softmax(score, dim=1)\n",
    "        \n",
    "        # Create the context vector\n",
    "        # Attention weights -> [b, max_seq_len, value]\n",
    "        # Image features -> [b, max_seq_len, embed_size]\n",
    "        # Context vector ->[b, value, embed_size]\n",
    "        context_vector = attn * image_features\n",
    "\n",
    "        context_vector = torch.sum(context_vector, 1)\n",
    "        context_vector = context_vector.view(context_vector.shape[0], -1)\n",
    "        \n",
    "        # Embedd the input sequence\n",
    "        embedding_sequence = self.embedding(inpt_sequence) # [B, 1, embed_size]\n",
    "        \n",
    "        x = torch.cat((embedding_sequence.view(embedding_sequence.shape[0], 1, -1), context_vector.view(context_vector.shape[0], 1, -1)), dim=2)\n",
    "        x = self.dropout(self.input_lin(x))\n",
    "        \n",
    "        x, hidden = self.gru(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(self.out1(x))\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.out2(x)\n",
    "        \n",
    "        # Softmax will be applied with the nn.Croessentropy loss\n",
    "        # no need to double softmax this boi\n",
    "        \n",
    "        return x, hidden\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1789, -0.5255,  0.0902,  ...,  0.0377, -0.0546,  0.1611],\n",
       "         [-0.4326, -1.1425,  0.7373,  ..., -0.4647, -0.2748, -1.4796],\n",
       "         [ 0.4928,  0.0301, -0.8610,  ..., -1.2885, -0.1576, -0.5311],\n",
       "         ...,\n",
       "         [-0.4320,  1.2746,  0.7269,  ..., -0.4191, -0.2051, -0.4628],\n",
       "         [-0.8408,  0.7471,  0.5631,  ..., -0.0415,  0.7361,  0.3495],\n",
       "         [-0.0396, -0.7126,  0.3778,  ..., -0.7192,  0.0017, -0.3393]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " tensor([[[ 0.0270,  0.0559,  0.1382,  ..., -0.0162, -0.2553, -0.1315],\n",
       "          [ 0.0917,  0.0926,  0.0941,  ..., -0.0593, -0.2707, -0.0671],\n",
       "          [ 0.0237,  0.0540,  0.0219,  ..., -0.0232, -0.2892, -0.1058],\n",
       "          ...,\n",
       "          [ 0.0066,  0.0607,  0.0097,  ..., -0.0563, -0.2388, -0.1141],\n",
       "          [ 0.0731,  0.0709,  0.0740,  ..., -0.0262, -0.2163, -0.0017],\n",
       "          [ 0.0590,  0.0355,  0.0999,  ..., -0.0205, -0.1912, -0.0389]]],\n",
       "        device='cuda:0', grad_fn=<CudnnRnnBackward>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = BahdanauAttentionDecoder(512, 512 ,1000).to(device)\n",
    "decoder(torch.zeros(32, 1).type(torch.LongTensor).cuda(), torch.zeros(32, 49, 512).to(device), torch.zeros(32, 512).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderToDecoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, dropout=0.10):\n",
    "        super(EncoderToDecoder, self).__init__()\n",
    "        \n",
    "        # Define the dimension size for the vocab, hidden and embedding\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define the encoder and the decoder network\n",
    "        self.encoder = EncoderCNN(embedding_size)\n",
    "        self.decoder = BahdanauAttentionDecoder(embedding_size, hidden_size , vocab_size)\n",
    "        \n",
    "    def init_hidden(self, batch):\n",
    "        return self.decoder.init_hidden(batch)\n",
    "        \n",
    "    def forward(self, inp, images, hidden):\n",
    "        \n",
    "        image_features = self.encoder(images)\n",
    "        \n",
    "        x, hidden = self.decoder(inp, image_features, hidden)\n",
    "        \n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load img\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "spacy_eng = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, dataset, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.dataset = dataset\n",
    "        seof.build_vocabulary()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        tokens = [tok.text.lower() for tok in spacy_eng.tokenizer(text)]  \n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file, sep=\"\\t\", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get img, caption columns\n",
    "        self.imgs = self.df[self.df.columns[0]]\n",
    "        self.captions = self.df[self.df.columns[1]]\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id.split('#')[0])).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=True,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
    ")\n",
    "\n",
    "loader, dataset = get_loader(\n",
    "    \"F:\\Flickr8k\\Flicker8k_Dataset\", \"F:\\Flickr8k\\Flickr8k.token.txt\", transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "batch = 64\n",
    "\n",
    "\n",
    "model = EncoderToDecoder(embedding_size, hidden_size , vocab_size).to(device)\n",
    "\n",
    "model_optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'a', 'party', 'party', 'of', 'off', 'bridesmaids', 'party', 'is', 'standing', 'a', 'and', 'a', 'animals', 'gathered', 'outside', '<EOS>', 'a', 'a', 'a', 'a']\n",
      "Loss after 25 batch iterations is 782.6465944449108\n",
      "['<SOS>', 'a', 'man', 'wearing', 'a', 'red', 'on', 'a', 'beach', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "Loss after 25 batch iterations is 757.9713344573975\n",
      "['<SOS>', 'three', 'women', 'posing', 'beers', ' ', 'posing', 'for', 'a', 'picture', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "Loss after 25 batch iterations is 749.9689463022593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-98efa2b6a005>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The loss for epoch {} is {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-132-98efa2b6a005>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;31m# Clip the gradients for the sequential models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        epoch_loss = 0\n",
    "        for idx, (imgs, captions) in enumerate(loader):   \n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            model_optimizer.zero_grad()\n",
    "            hidden = model.init_hidden(batch)\n",
    "            result_batch = [1]\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(1, captions.shape[1]):\n",
    "                out, hidden = model(captions[:, i-1], imgs, hidden.view(batch, -1))\n",
    "\n",
    "                topv, topi = out.topk(1)\n",
    "                result_batch.append(topi[5].item())\n",
    "                loss += criterion(out, captions[:, i])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward() \n",
    "            # Clip the gradients for the sequential models\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            model_optimizer.step()\n",
    "\n",
    "            if (idx+1) % 25  == 0 :\n",
    "                result_string = [dataset.vocab.itos[x] if x in dataset.vocab.itos else dataset.vocab.itos[3] for x in result_batch]\n",
    "                print(result_string)\n",
    "                print('Loss after 25 batch iterations is {}'.format(epoch_loss / idx * 25))\n",
    "        \n",
    "        print('The loss for epoch {} is {}'.format(epoch, epoch_loss / len(loader.dataset)))\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderToDecoder(\n",
       "  (encoder): EncoderCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): BahdanauAttentionDecoder(\n",
       "    (Qattn): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (Kattn): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (Vattn): Linear(in_features=512, out_features=1, bias=False)\n",
       "    (out1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (embedding): Embedding(8831, 512)\n",
       "    (hidden_concat): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (gru): GRU(512, 512, batch_first=True)\n",
       "    (input_lin): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (out2): Linear(in_features=512, out_features=8831, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "tensor([[2]], device='cuda:0')\n",
      "tensor([[1952]], device='cuda:0')\n",
      "tensor([[199]], device='cuda:0')\n",
      "tensor([[13]], device='cuda:0')\n",
      "tensor([[200]], device='cuda:0')\n",
      "['<SOS>', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people', '<EOS>', 'league', 'group', 'of', 'people']\n"
     ]
    }
   ],
   "source": [
    "def test_sentences():\n",
    "    with torch.no_grad():\n",
    "        horse = transform(Image.open(os.path.join(os.path.abspath('F:\\\\Flickr8k\\\\test'), 'cal.jpg')).convert(\"RGB\"))\n",
    "        dog = transform(Image.open(os.path.join(os.path.abspath('F:\\\\Flickr8k\\\\test'), 'dog.jpg')).convert(\"RGB\"))\n",
    "        girl = transform(Image.open(os.path.join(os.path.abspath('F:\\\\Flickr8k\\\\test'), 'girl4.png')).convert(\"RGB\"))\n",
    "        boat = transform(Image.open(os.path.join(os.path.abspath('F:\\\\Flickr8k\\\\test'), 'boat.png')).convert(\"RGB\"))\n",
    "        out = torch.ones(1, 1).type(torch.LongTensor).to(device)\n",
    "        result = [1]\n",
    "\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        for i in range(50):\n",
    "            out, hidden = model(out, boat.view(1, horse.shape[0], horse.shape[1], horse.shape[2]).to(device), hidden.view(1, -1))\n",
    "            topv, topi = out.topk(1)\n",
    "            \n",
    "            print(topi)\n",
    "            \n",
    "            out = topi.view(-1, 1)\n",
    "            result.append(out.item())\n",
    "        \n",
    "    result_string = [dataset.vocab.itos[x] if x in dataset.vocab.itos else dataset.vocab.itos[3] for x in result]\n",
    "    print(result_string)\n",
    "\n",
    "test_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"C:\\\\Users\\\\Cordu\\\\Desktop\\\\Projects\\\\Artificial-Intelligence\\\\RIST PROJECTS\\\\NLP BEGINNER\\\\model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
