{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    def __init__(self, FILE_PATH):\n",
    "        self.file = open(FILE_PATH, encoding=\"utf8\")\n",
    "        # Word index for input/target\n",
    "        self.word2index_input = {}\n",
    "        self.word2index_target = {}\n",
    "        # Word count for input/target\n",
    "        self.word2count_input = {}\n",
    "        self.word2count_target = {}\n",
    "        # Word for the index \n",
    "        self.index2word_input = {}\n",
    "        self.index2word_target = {}\n",
    "        # Create training data\n",
    "        self.training_data = []\n",
    "        self.read_file()\n",
    "        self.count_words()\n",
    "        self.create_dictionaries()\n",
    "        # Final dataset structure\n",
    "        self.training_set = []\n",
    "        self.create_training_set()\n",
    "    \n",
    "    def read_file(self):\n",
    "        for line in self.file.readlines():\n",
    "            line = line.split('\\t')[0:2] # split \n",
    "            line = [word.translate(str.maketrans('', '', string.punctuation)) for word in line]\n",
    "            line = [word.lower() for word in line]\n",
    "            self.training_data.append((line[0], line[1]))\n",
    "    \n",
    "    def count_words(self):\n",
    "        for line, translation in self.training_data:\n",
    "            for word in line.split(' '):\n",
    "                if word in self.word2count_input:\n",
    "                    self.word2count_input[word] = self.word2count_input[word] + 1\n",
    "                else:\n",
    "                    self.word2count_input[word] = 1\n",
    "            for word in translation.split(' '):\n",
    "                if word in self.word2count_target:\n",
    "                    self.word2count_target[word] = self.word2count_target[word] + 1\n",
    "                else:\n",
    "                    self.word2count_target[word] = 1\n",
    "    \n",
    "    def create_dictionaries(self):\n",
    "        self.word2index_input['SOS'] = 0\n",
    "        self.word2index_input['EOS'] = 1\n",
    "        self.word2index_input['unk'] = 2\n",
    "        self.index2word_input[0] = 'SOS'\n",
    "        self.index2word_input[1] = 'EOS'\n",
    "        self.index2word_input[2] = 'unk'\n",
    "        \n",
    "        self.word2index_target['SOS'] = 0\n",
    "        self.word2index_target['EOS'] = 1\n",
    "        self.word2index_target['unk'] = 2\n",
    "        self.index2word_target[0] = 'SOS'\n",
    "        self.index2word_target[1] = 'EOS'\n",
    "        self.index2word_target[2] = 'unk'\n",
    "        \n",
    "        for line, translation in self.training_data:\n",
    "            for word in line.split(' '):\n",
    "                if self.word2count_input[word] > 4:\n",
    "                    if word not in self.word2index_input:\n",
    "                        self.word2index_input[word] = len(self.word2index_input)\n",
    "                        self.index2word_input[len(self.word2index_input) - 1] = word\n",
    "            for word in translation.split(' '):\n",
    "                if self.word2count_target[word] > 4:\n",
    "                    if word not in self.word2index_target:\n",
    "                        self.word2index_target[word] = len(self.word2index_target)\n",
    "                        self.index2word_target[len(self.word2index_target) - 1] = word\n",
    "    \n",
    "    def create_training_set(self):\n",
    "        for line, translation in self.training_data:\n",
    "            inp = []\n",
    "            for word in line.split(' '):\n",
    "                if word in self.word2index_input:\n",
    "                    inp.append(self.word2index_input[word])\n",
    "                else:\n",
    "                    inp.append(self.word2index_input['unk'])\n",
    "            target = []\n",
    "            for word in translation.split(' '):\n",
    "                if word in self.word2index_target:\n",
    "                    target.append(self.word2index_target[word])\n",
    "                else:\n",
    "                    target.append(self.word2index_target['unk'])\n",
    "            # append EOS to the sentences\n",
    "            inp.append(1)\n",
    "            target.append(1)\n",
    "            self.training_set.append((torch.tensor(inp), torch.tensor(target)))\n",
    "            \n",
    "util = Util(os.path.abspath('ron.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2, 1]), tensor([3, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(util.training_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_hidden, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_hidden = emb_hidden\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_hidden)\n",
    "        self.dense = nn.Linear(self.emb_hidden, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size).to(device), torch.zeros(1, 1, self.hidden_size).to(device))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x = self.dense(x)\n",
    "        x = F.relu(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_hidden, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_hidden = emb_hidden\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_hidden)\n",
    "        self.dense = nn.Linear(self.emb_hidden, self.hidden_size)\n",
    "        self.attn = nn.Linear(3 * hidden_size, 1)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.input_combine = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.last = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "    \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size).to(device), torch.zeros(1, 1, self.hidden_size).to(device))\n",
    "    \n",
    "    \n",
    "    def forward(self, x, decoder_hidden, encoder_outputs):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x = self.dense(x)\n",
    "        x = F.relu(x)\n",
    "        attention_values = []\n",
    "        for i in range(len(encoder_outputs)):\n",
    "            # concatinate encoder_output at i with the decoder hidden state (tuple cause lstm has 2) -> 3 * hidden_size \n",
    "            enc_dec_concat = torch.cat((encoder_outputs[i].view(1, 1, -1), torch.cat((decoder_hidden[0], decoder_hidden[1]), 2)), 2)\n",
    "            attn_value = self.attn(enc_dec_concat)\n",
    "            attention_values.append(attn_value)\n",
    "        alphas = torch.cat(attention_values, 1)\n",
    "        alphas_norm = F.softmax(alphas, dim=1)\n",
    "        # Weight multiplication for each encoder output to denote it's importance \n",
    "        c = torch.bmm(alphas_norm.view(1, 1, -1), encoder_outputs.view(1, -1, self.hidden_size))\n",
    "        \n",
    "        x = torch.cat((x.view(1, 1, -1), c.view(1, 1, -1)), 2)\n",
    "        x = self.input_combine(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        out, decoder_hidden = self.lstm(x, decoder_hidden)\n",
    "        out = self.last(out[0])\n",
    "        \n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out, decoder_hidden\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(util.word2index_input), 50, 256).to(device)\n",
    "decoder = Decoder(len(util.word2index_target), 50, 256).to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(input_tensor, translation_tensor):\n",
    "    hidden_enc = encoder.init_hidden()\n",
    "    hidden_dec = decoder.init_hidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outputs = torch.zeros((50, 1, 256)).to(device)# max 10 words per sequence\n",
    "    teacher_enforcing = random.random() > 0.5 # teacher enforcing\n",
    "    loss = 0\n",
    "    \n",
    "    # Encoding part\n",
    "    for i in range(len(input_tensor)):\n",
    "        encoder_out, hidden_enc = encoder(input_tensor[i].to(device), hidden_enc)\n",
    "        encoder_outputs[i] = encoder_out\n",
    "\n",
    "    out = torch.tensor(util.word2index_input['SOS']).to(device)\n",
    "    \n",
    "    if teacher_enforcing:\n",
    "        for i in range(len(translation_tensor)):\n",
    "            translation_out, hidden_dec = decoder(out.to(device), hidden_dec, encoder_outputs)\n",
    "            out = torch.tensor(translation_tensor[i].to(device))\n",
    "            loss += criterion(translation_out, translation_tensor[i].unsqueeze(0).to(device))\n",
    "            if out.item() == 'EOS':\n",
    "                break\n",
    "    else:\n",
    "        for i in range(len(translation_tensor)):\n",
    "            translation_out, hidden_dec = decoder(out.to(device), hidden_dec, encoder_outputs)\n",
    "            topv, topi = translation_out.topk(1)\n",
    "            out = topi.detach().long().cuda()\n",
    "            loss += criterion(translation_out, translation_tensor[i].unsqueeze(0).to(device))\n",
    "            if out.item() == 'EOS':\n",
    "                break\n",
    "            \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "        \n",
    "    return loss / len(translation_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(input_tensor, target_tensor):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    hidden_enc = encoder.init_hidden()\n",
    "    encoder_outputs = torch.zeros((50, 1, 256)).to(device) # max 50 words per sequence\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, word in enumerate(input_tensor):\n",
    "            out, hidden_enc = encoder(word.to(device), hidden_enc)\n",
    "            encoder_outputs[i] = out\n",
    "\n",
    "        hidden_dec = hidden_enc\n",
    "        decoder_input = torch.tensor([util.word2index_input['SOS']]).to(device)\n",
    "        \n",
    "        for i, word in enumerate(target_tensor):\n",
    "            total += 1\n",
    "            out, hidden_dec = decoder(decoder_input.to(device), hidden_dec, encoder_outputs)\n",
    "            topv, topi = out.topk(1)\n",
    "            decoder_input = topi.detach().long().cuda()\n",
    "            if decoder_input.item() == target_tensor[i].item():\n",
    "                correct += 1\n",
    "            if decoder_input.item() == 0:\n",
    "                if target_tensor[i] == 0:\n",
    "                    correct +=1\n",
    "                break\n",
    "        \n",
    "    \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cordu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfc6b185bbc4eab8a4b989ede65e8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10449.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-80386b843a6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-80386b843a6b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-8543b2748134>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(input_tensor, translation_tensor)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for i in range(10):\n",
    "        loss = 0\n",
    "        for sentence, translation in tqdm_notebook(util.training_set):\n",
    "            loss += learn(sentence, translation)\n",
    "        \n",
    "        \n",
    "        print(\"Loss for this bad boi at 10 epochs is : {}\".format(loss // len(util.training_set)))\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for sentence, translation in tqdm_notebook(util.training_set):\n",
    "            correct_it, total_it = validate(sentence, translation)\n",
    "            correct += correct_it\n",
    "            total += total_it\n",
    "        print(\"Accuracy for this bad boi is : {}\".format((correct / total) * 100))\n",
    "            \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
